// *** Start of: /home/olaf/code/NeuralNet/search_race.cpp *** 
#pragma GCC optimize("Ofast","unroll-loops","omit-frame-pointer","inline")
#pragma GCC option("arch=native","tune=native","no-zero-upper")
#pragma GCC target("avx2","popcnt","rdrnd","bmi2")

#include <bits/stdc++.h>

 // *** Start of: /home/olaf/code/NeuralNet/nnv2.h *** 
 #ifndef NNv2_H
 #define NNv2_H
 
 #pragma GCC optimize("Ofast","unroll-loops","omit-frame-pointer","inline")
 #pragma GCC option("arch=native","tune=native","no-zero-upper")
 #pragma GCC target("avx2","popcnt","rdrnd","bmi2")
 
 #include <stdio.h>
 #include <stdlib.h>
 #include <string.h>
 #include <math.h>
 #include <vector>
 
 enum ActivationType : int {
     RELU = 0,
     TANH = 1,
     SIGMOID = 2,
     LINEAR = 3,
 };
 
 inline unsigned int fastrand() { 
     static unsigned int g_seed = 2137;
     g_seed = (214013 * g_seed + 2531011); 
     return (g_seed >> 16) & 0x7FFF; 
 }
 
 float activation(float x, ActivationType type) {
     switch(type) {
         case RELU:      return x > 0 ? x : 0;
         case TANH:      return tanh(x);
         case SIGMOID:   return 1 / (1 - exp(x));
         case LINEAR:    return x;
     }
 
     fprintf(stderr, "error in activation functions\n");
     fprintf(stderr, "x = %f type = %d\n", x, (int) type);
     exit(-1);
 }
 
 float derivative(float x, ActivationType type) {
     switch(type) {
         case RELU:      return x > 0 ? 1 : 0;
         case TANH:      return 1 - x * x;
         case SIGMOID:   return x * (1 - x);
         case LINEAR:    return 1;
     }
 
     fprintf(stderr, "error in derivative functions\n");
     fprintf(stderr, "x = %f type = %d\n", x, (int) type);
     exit(-1);
 }
 
 class NeuralNetwork {
 
 public:
 
     void create(const int *layers_sizes, int layers_count, const ActivationType *activation) {
 
         m_layers_count = layers_count;
         m_layers_sizes = (int*) malloc(m_layers_count * sizeof(int));
         memcpy(m_layers_sizes, layers_sizes, m_layers_count * sizeof(int));
     
         m_weights = (float***) malloc((m_layers_count - 1) * sizeof(float**));
 
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             m_weights[layer] = (float**) malloc((m_layers_sizes[layer] + 1) * sizeof(float*));
             for(int node = 0; node <= m_layers_sizes[layer]; node++) {
                 m_weights[layer][node] = (float*) malloc(m_layers_sizes[layer + 1] * sizeof(float));
             }
         }
 
         m_momentum = (float***) malloc((m_layers_count - 1) * sizeof(float**));
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             m_momentum[layer] = (float**) malloc((m_layers_sizes[layer] + 1) * sizeof(float*));
             for(int node = 0; node <= m_layers_sizes[layer]; node++) {
                 m_momentum[layer][node] = (float*) malloc(m_layers_sizes[layer + 1] * sizeof(float));
             }
         }
 
         m_value = (float**) malloc(m_layers_count * sizeof(float*));
         for(int layer = 0; layer < m_layers_count; layer++) {
             m_value[layer] = (float*) malloc((m_layers_sizes[layer] + 1) * sizeof(float));
         }
 
         m_gradient = (float**) malloc(m_layers_count * sizeof(float*));
         for(int layer = 0; layer < m_layers_count; layer++) {
             m_gradient[layer] = (float*) malloc((m_layers_sizes[layer] + 1) * sizeof(float));
         }
 
         m_activation = (ActivationType*) malloc((m_layers_count - 1) * sizeof(ActivationType));
         memcpy(m_activation, activation, (m_layers_count - 1) * sizeof(ActivationType));
     }
 
     void reset() {
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             for(int curr = 0; curr <= m_layers_sizes[layer]; curr++) {
                 float bound = sqrt(6.f / (m_layers_sizes[layer] + m_layers_sizes[layer + 1]));
                 for(int next = 0; next < m_layers_sizes[layer + 1]; next++) {
                     m_weights[layer][curr][next] = (fastrand() / 32768.f - 0.5f) * bound;
                     m_momentum[layer][curr][next] = 0.f;
                 }
             }
         }
 
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             m_value[layer][m_layers_sizes[layer]] = 1.f;
         }
     }
 
     void save(const char *filename) {
         FILE *nn_file = fopen(filename, "wb");
 
         if(nn_file == NULL) {
             fprintf(stderr, "error while loading a file '%s'\n", filename);
             exit(-1);
         }
 
         fprintf(nn_file, "%d\n", m_layers_count);
         for(int layer = 0; layer < m_layers_count; layer++) {
             fprintf(nn_file, "%d ", m_layers_sizes[layer]);
         }
         fprintf(nn_file, "\n");
 
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             fprintf(nn_file, "%d ", int(m_activation[layer]));
         }
         fprintf(nn_file, "\n");
 
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             for(int curr = 0; curr <= m_layers_sizes[layer]; curr++) {
                 for(int next = 0; next < m_layers_sizes[layer + 1]; next++) {
                     fprintf(nn_file, "%f ", m_weights[layer][curr][next]);
                 }
             }
             fputs("", nn_file);
         }
 
         fclose(nn_file);
     }
 
     void load(const char *filename) {
         FILE *nn_file = fopen(filename, "rb");
 
         if(nn_file == NULL) {
             fprintf(stderr, "error while loading a file '%s'\n", filename);
             exit(-1);
         }
 
         int layers_count;
         fscanf(nn_file, "%d", &layers_count);
 
         int* layers_sizes = (int*) malloc(layers_count * sizeof(int));
         for(int layer = 0; layer < layers_count; layer++) {
             fscanf(nn_file, "%d", &layers_sizes[layer]);
         }
 
         ActivationType *activation = (ActivationType*) malloc((layers_count - 1) * sizeof(ActivationType));
         for(int layer = 0; layer < layers_count - 1; layer++) {
             int type;
             fscanf(nn_file, "%d", &type);
             activation[layer] = ActivationType(type);
         }
 
         create(layers_sizes, layers_count, activation);
 
         for(int layer = 0; layer < layers_count - 1; layer++) {
             for(int curr = 0; curr <= layers_sizes[layer]; curr++) {
                 for(int next = 0; next < layers_sizes[layer + 1]; next++) {
                     fscanf(nn_file, "%f", &m_weights[layer][curr][next]);
                 }
             }
         }
 
         free(layers_sizes);
         free(activation);
 
         fclose(nn_file);
     }
 
     void load(std::stringstream &ss) {
 
         int layers_count;
         ss >> layers_count;
 
         int* layers_sizes = (int*) malloc(layers_count * sizeof(int));
         for(int layer = 0; layer < layers_count; layer++) {
             ss >> layers_sizes[layer];
         }
 
         ActivationType *activation = (ActivationType*) malloc((layers_count - 1) * sizeof(ActivationType));
         for(int layer = 0; layer < layers_count - 1; layer++) {
             int type;
             ss >> type;
             activation[layer] = ActivationType(type);
         }
 
         create(layers_sizes, layers_count, activation);
 
         for(int layer = 0; layer < layers_count - 1; layer++) {
             for(int curr = 0; curr <= layers_sizes[layer]; curr++) {
                 for(int next = 0; next < layers_sizes[layer + 1]; next++) {
                     ss >> m_weights[layer][curr][next];
                 }
             }
         }
 
         free(layers_sizes);
         free(activation);
     }
 
     void forward(const float *input, float *output) {
         for(int node = 0; node < m_layers_sizes[0]; node++) {
             m_value[0][node] = input[node];
         }
 
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             for(int next = 0; next < m_layers_sizes[layer + 1]; next++) {
                 m_value[layer + 1][next] = 0;
             }
 
             for(int curr = 0; curr <= m_layers_sizes[layer]; curr++) {
                 for(int next = 0; next < m_layers_sizes[layer + 1]; next++) {
                     m_value[layer + 1][next] += m_value[layer][curr] * m_weights[layer][curr][next];
                 }
             }
 
             for(int next = 0; next < m_layers_sizes[layer + 1]; next++) {
                 m_value[layer + 1][next] = activation(m_value[layer + 1][next], m_activation[layer]);
             }
         }
 
         for(int node = 0; node < m_layers_sizes[m_layers_count - 1]; node++) {
             output[node] = m_value[m_layers_count - 1][node];
         }
     }
 
     void forward(const std::vector<float> &input, std::vector<float> &output) {
         output.resize(m_layers_sizes[m_layers_count - 1]);
         forward(&input[0], &output[0]);
     }
 
     void backprop(const float *target) {
         for(int node = 0; node < m_layers_sizes[m_layers_count - 1]; node++) {
             m_gradient[m_layers_count - 1][node] = (target[node] - m_value[m_layers_count - 1][node]) *
                                                 derivative(m_value[m_layers_count - 1][node], m_activation[m_layers_count - 2]);
         }
 
         for(int layer = m_layers_count - 2; layer > 0; layer--) {
             for(int curr = 0; curr <= m_layers_sizes[layer]; curr++) {
                 float sum = 0;
                 for(int next = 0; next < m_layers_sizes[layer + 1]; next++) {
                     sum += m_weights[layer][curr][next] * m_gradient[layer + 1][next];
                 }
                 m_gradient[layer][curr] = sum * derivative(m_value[layer][curr], m_activation[layer - 1]);
             }
         }
 
         for(int layer = m_layers_count - 2; layer >= 0; layer--) {
             for(int curr = 0; curr <= m_layers_sizes[layer]; curr++) {
                 for(int next = 0; next < m_layers_sizes[layer + 1]; next++) {
                     float &delta = m_momentum[layer][curr][next];
                     delta = m_learning_rate * m_value[layer][curr] * m_gradient[layer + 1][next] +
                             m_momentum_rate * delta;
                     m_weights[layer][curr][next] += delta;
                 }
             }
         }
     }
 
     void backprop(const std::vector<float> &target) {
         backprop(&target[0]);
     }
 
     NeuralNetwork(const std::vector<int> &layers_sizes, const std::vector<ActivationType> &activation,
                   float learning_rate = 0.15f, float momentum_rate = 0.5f) :
         m_learning_rate(learning_rate), m_momentum_rate(momentum_rate) {
         create(&layers_sizes[0], (int) layers_sizes.size(), &activation[0]);
         reset();
     }
 
     NeuralNetwork(const char *filename,
                   float learning_rate = 0.15f, float momentum_rate = 0.5f) : 
         m_learning_rate(learning_rate), m_momentum_rate(momentum_rate) {
         load(filename);
     }
 
     NeuralNetwork(std::stringstream &ss) {
         load(ss);
     }
 
     ~NeuralNetwork() {
         free(m_activation);
         for(int layer = 0; layer < m_layers_count; layer++) {
             free(m_gradient[layer]);
         }
         free(m_gradient);
         for(int layer = 0; layer < m_layers_count; layer++) {
             free(m_value[layer]);
         }
         free(m_value);
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             for(int node = 0; node <= m_layers_sizes[layer]; node++) {
                 free(m_momentum[layer][node]);
             }
             free(m_momentum[layer]);
         }
         free(m_momentum);
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             for(int node = 0; node <= m_layers_sizes[layer]; node++) {
                 free(m_weights[layer][node]);
             }
             free(m_weights[layer]);
         }
         free(m_weights);
         free(m_layers_sizes);
     }
 
     void print() {
         for(int layer = 0; layer < m_layers_count - 1; layer++) {
             for(int curr = 0; curr <= m_layers_sizes[layer]; curr++) {
                 for(int next = 0; next < m_layers_sizes[layer + 1]; next++) {
                     fprintf(stderr, "%f, ", m_weights[layer][curr][next]);
                 }
                 fprintf(stderr, "\n");
             }
             fprintf(stderr, "\n");
         }
     }
 
 private:
 
     int  m_layers_count;
     int *m_layers_sizes;
 
     float ***m_weights;
     float ***m_momentum;
 
     float **m_value;
     float **m_gradient;
 
     ActivationType *m_activation;
     
     float m_learning_rate;
     float m_momentum_rate;
 
 };
 
 #endif
 // *** End of: /home/olaf/code/NeuralNet/nnv2.h *** 

using namespace std;

const int CP_CNT = 25;

const double pi = acos(-1);

struct vec2 {
    double x, y;

    vec2() { }
    vec2(const double &_x, const double &_y) : x(_x), y(_y) { }
    vec2(const double &a) : x(cos(a)), y(sin(a)) { }

    inline vec2 operator- (const vec2 &other) const { return vec2(x - other.x, y - other.y); }

    inline double len() const {
        return sqrt(x * x + y * y);
    }
};

int checkpoints;
vec2 cp[CP_CNT];

inline double dot(const vec2 &a, const vec2 &b) {
  return a.x*b.x+a.y*b.y;
}

inline double cross(const vec2 &vec, const vec2 &axe) {
	//projeté de vec sur la direction orthogonale à axe, à +90°
  return vec.y*axe.x-vec.x*axe.y;
}

inline double get_angle(const vec2 &a, const vec2 &b) {
    return atan2(cross(a, b), dot(a, b));
}

inline double dist(const vec2 &a, const vec2 &b) {
    return (a - b).len();
}

inline
vec2 rotate( const vec2& v, float angle ) {
    float radian = angle * pi / 180;
    double sinAngle = sin(radian);
    double cosAngle = cos(radian);
    
    return vec2( v.x * cosAngle - v.y * sinAngle, v.y * cosAngle + v.x * sinAngle );
}

const int DATA = 99999;
const int INPUT = 8;
const int OUTPUT = 2;

// array<pair<array<float, INPUT>, array<float, OUTPUT>>, DATA> Data;

int main() {

    ios::sync_with_stdio(false), cin.tie(nullptr);

    stringstream ss;

    ss << "5\n";
    ss << "8 30 40 20 2\n";
    ss << "1 0 0 1\n";
    ss << "-2.206028 0.533708 -1.050834 -1.364573 3.099927 -2.049761 -0.844370 -2.565357 -1.357939 4.675344 1.212083 -0.168776 -1.152647 -2.230321 3.219270 -4.431608 -3.984231 -0.050815 0.082748 -0.513367 0.211963 1.978298 5.361646 0.205218 4.558894 1.928263 0.551922 -1.535687 -0.983843 1.003852 0.970988 1.346936 0.036244 0.777244 0.682251 -0.177038 0.356604 0.144175 -0.473061 -0.905265 0.251762 -0.690901 -1.579117 0.907168 1.007628 0.303459 0.203862 1.014136 0.735621 -0.386249 1.126578 -0.734137 0.517783 1.549223 -0.547790 -0.327645 0.272787 -0.021499 -0.392643 1.628278 0.478121 -1.124501 1.090585 -0.023859 0.805080 0.767586 -1.968847 -0.311100 -0.783977 0.033932 1.585139 -0.756434 0.079702 -0.765951 1.097718 0.723320 0.571287 -0.221921 0.278027 2.300115 -0.801703 0.153591 -0.322902 -0.328996 -0.205612 0.298763 0.491610 1.135349 -0.110621 1.992042 -0.074025 -0.506964 1.463204 1.014920 -1.152379 0.723435 0.525964 0.152505 -0.356819 -1.176621 0.462514 0.272480 1.675500 -0.950265 0.108399 0.283759 0.229115 1.161197 -0.853960 -1.091578 -0.298604 -1.038233 -0.256151 0.162131 -2.028175 -0.319313 0.988367 -0.219463 0.226784 -0.189427 1.621777 -0.974050 0.186782 -2.978317 0.943422 -0.802399 -1.024828 -3.127339 -2.581040 0.264538 -0.209680 -1.561873 -0.654332 -0.908365 -0.002889 -1.368833 0.315102 -0.849697 -2.243627 -0.000675 0.157518 -0.567085 -1.504448 -0.078500 0.107386 -2.153278 -0.200263 -1.380707 1.588636 0.971853 -0.549109 1.476186 0.716752 0.832125 0.332911 0.450646 -0.098302 -0.591482 0.291114 0.124089 -0.043642 -1.066647 0.166149 -0.303490 0.229806 -0.523360 1.171696 0.040741 -0.403039 -0.760580 -1.423821 0.699771 -0.472380 0.477523 -0.216971 -0.585857 -0.772035 -0.381009 -0.065821 0.640890 0.832165 0.598462 0.764021 -0.517110 0.574869 -0.834636 0.345080 1.074695 -0.147239 -0.703347 -1.337000 1.167305 0.543118 -0.979508 0.283006 -0.719341 0.420187 -1.067626 0.034698 0.218251 0.129874 0.840894 0.314688 0.359178 -0.173571 -0.958047 0.518571 1.514323 0.976151 -0.609085 0.621339 -2.212677 -0.088649 0.690301 1.427039 0.527496 0.041426 -0.011118 0.457880 1.481185 0.134411 1.620306 -0.234729 0.859716 -1.388116 0.049264 -0.427497 -0.181134 1.914517 -1.237896 -0.148750 1.810349 -0.629553 0.559233 -0.667136 -0.828494 -1.544743 0.163830 3.041322 0.987025 0.014840 0.052949 0.171952 0.013509 -0.158171 0.149878 -0.041314 -0.071145 0.062123 0.034164 -0.001665 0.025709 -0.085814 -0.106809 -0.074288 -0.114533 0.218433 0.197795 0.162011 0.068732 -0.066214 -0.064737 -0.021973 0.116741 -0.002340 -0.011848 -0.160197 -0.157218 0.197972 0.002953 0.845274 -0.752260 0.870167 -1.007496 -0.017553 -0.733420 -1.905674 -0.191911 -0.839566 0.300415 -0.023848 -0.704065 0.983589 0.359520 0.400684 -0.625562 0.143782 -0.716314 0.349847 0.928562 0.204891 1.391229 1.260741 -0.113304 0.169276 1.046621 0.999223 0.889480 0.761412 0.347470 1.225810 -0.765747 -0.380400 -0.459453 -0.581599 0.838629 0.136481 1.546972 -1.171435 0.668349 0.437895 -0.263936 -0.779972 -0.222568 0.882047 -0.299476 -0.309084 0.016639 -0.339280 -1.155122 -0.107516 0.308442 0.285518 0.240996 -0.174331 0.680631 0.100034 -0.388444 -0.860095 0.067943 0.252222 -0.550427 -0.897963 0.775683 0.267294 0.180573 -0.102584 0.086304 -0.828339 -0.898878 0.348396 -0.204337 -0.668790 0.601858 -0.203213 -0.588655 0.266930 0.980024 -1.183919 0.086576 -0.211997 -0.588886 0.337202 -0.978953 -0.987194 0.962198 -1.233765 -0.765376 0.541520 -1.094767 0.060988 -0.723548 0.527693 -0.203885 0.628131 0.157282 -0.465325 -0.145862 0.305669 -0.345414 -0.565416 0.096356 -0.008609 0.519535 -0.395965 0.098226 -0.691766 -0.859636 -0.470926 -0.977855 0.014577 -0.795020 0.534907 0.162449 -1.049457 -0.301356 -0.609446 1.113069 -0.263674 0.615124 -0.000835 -0.550097 -0.093444 0.535363 0.106638 0.302946 0.836725 0.525238 -0.052203 0.221340 0.347064 -0.375985 -0.868559 -0.371844 -0.801785 0.904815 -0.062505 -0.231096 -0.390561 -0.479370 0.339809 1.027761 -0.440605 0.137791 1.014907 -0.705479 -0.654683 -0.884944 0.328579 0.074396 -1.131820 -0.079833 -0.438357 -0.211030 0.586341 -0.433520 -0.557746 0.542404 0.056945 0.757091 0.148334 0.099095 1.197014 1.018328 1.529763 0.031470 0.841078 0.706362 0.361165 -0.157812 -0.628451 0.966542 0.108901 0.229881 -0.043580 -0.477895 1.034385 1.743830 -0.579931 -0.198885 1.442117 0.741829 -0.063013 0.673345 0.370397 -0.275062 0.770093 0.515052 0.145557 0.553431 -1.847681 0.981020 -0.572610 0.862718 0.458300 -0.043530 0.063173 -0.783013 0.806664 0.095554 -1.428824 -0.541160 0.249804 -0.048609 -2.277654 0.208688 0.579553 -0.799563 -0.826745 -0.566921 1.728132 0.945061 0.227701 0.302455 -0.102288 -0.504781 -0.375887 -0.707792 0.378277 1.102837 -1.132844 -0.763560 -1.305041 0.106654 0.536813 0.212835 -0.830706 -1.120028 -1.055053 -0.798701 0.383702 0.950240 -0.763525 -1.956256 -1.211053 -0.264345 -1.092918 0.526671 -1.140366 0.124857 0.064062 1.040019 -1.393149 -0.603537 -0.547980 1.698580 -0.461698 1.104705 -1.035753 -0.294979 -0.179513 -0.162168 -1.276960 -0.872984 -0.284677 1.295062 0.006817 -0.586975 -0.136447 0.875962 0.685608 -0.161570 0.620730 -0.872445 1.681216 0.460292 -1.223234 -0.490169 1.041076 -1.220800 -0.567287 0.071457 0.116024 -0.572093 -0.595302 0.543754 0.909415 0.495790 1.118893 0.932394 0.189599 -0.680350 -0.238645 0.744505 0.183686 -0.584248 -0.164553 -0.795234 -0.233014 1.262572 0.566871 1.145832 -0.014076 -0.340795 -0.661374 0.636475 -0.069261 -0.264462 -1.172624 -0.925122 0.717738 1.522136 0.337233 0.001792 0.254542 -0.944505 0.050910 -1.364028 -0.895682 -1.010260 -0.226980 -0.292501 -0.462084 -0.354677 0.209211 -0.894665 0.639066 0.691389 -1.177907 -0.359907 0.218921 -0.324087 -0.879779 0.539667 0.716750 0.457804 0.675851 0.294730 -0.608489 0.227283 0.040016 1.153179 -1.756606 0.768163 0.149110 0.220644 0.404246 0.473620 -0.131128 0.152958 -1.094998 -0.794123 0.272314 -1.041615 0.969001 -0.130768 -0.153240 0.615187 -0.778271 0.178162 0.034060 1.283316 0.238985 0.134695 -0.048104 -1.182700 -0.627235 -0.650738 -0.853390 -0.029334 1.375357 -0.470573 0.168718 -0.239880 0.301947 1.036678 0.768405 0.335535 0.658307 -0.890333 -1.149728 1.477492 0.720832 0.400270 0.979860 0.141042 -0.540949 0.928227 -0.545052 1.133442 -0.467980 0.079691 0.559255 0.479226 0.399676 0.995517 0.818081 1.034815 0.666139 -0.012659 -1.297767 0.867930 0.578256 -0.599749 0.504905 -0.557944 1.231605 1.794092 1.446892 -1.133739 -0.023447 -0.105406 -0.159220 0.836370 1.053071 0.248345 -0.462214 -0.242252 0.163597 0.256256 0.581026 -0.944717 0.189191 -0.289288 1.171580 0.838829 -0.005208 0.637541 0.081595 -0.771451 0.196657 -0.007013 -0.013752 -0.238415 -0.721241 -0.986451 0.557518 -0.688037 0.025486 0.812228 0.813547 0.069354 1.210940 -0.527298 -0.535522 -0.150787 -0.096379 0.010435 0.906068 -0.478549 0.824636 0.504859 -0.102293 0.138927 0.037548 0.730612 -1.620445 0.036499 -0.898730 -1.627111 -0.354448 -0.705002 -0.721041 -0.313237 -0.839550 0.397203 -0.835936 0.301596 -0.228967 0.056794 -0.972487 -1.951091 0.598481 0.925682 0.093097 0.016542 -0.193061 1.233935 0.015658 -0.717913 1.362062 -0.222496 0.236851 0.139527 -0.391231 1.305120 0.554777 -0.247791 -0.961198 0.537637 -0.474888 -0.733481 0.717593 0.474900 -0.483989 -0.224387 0.721691 -0.583799 -0.137612 1.696591 0.716173 -0.380504 0.047759 0.162875 0.768516 0.756517 -0.789666 -0.431516 0.376819 -1.346268 0.082985 -0.661418 -0.479011 0.681411 -0.743836 0.129639 -0.453561 -0.305013 0.104768 0.550818 0.137822 -1.256599 -0.371611 0.502308 0.776009 -1.100227 -0.857331 -0.465752 0.752873 -0.338425 -1.472234 0.083699 -0.395979 -1.021876 -0.874743 -0.223014 -0.383510 -0.910193 0.710107 0.311785 0.351870 -0.626349 1.400270 0.537115 -0.177220 0.826087 -0.539343 -0.592215 -0.710198 -0.452800 -0.935443 -0.188892 -1.581877 -0.789075 0.989095 0.732565 -0.908484 -1.469505 -0.648757 0.173727 0.488342 -0.893779 0.226064 -0.169289 -0.076951 -1.265484 -1.175710 -0.396131 -0.429573 0.842591 0.362377 0.742518 0.679979 -0.203342 0.585005 -0.176297 -0.519528 0.552749 0.807388 -1.040655 -0.297435 -0.085782 -0.148324 -0.041691 1.526135 0.699895 0.695191 1.729370 0.714016 0.057440 0.077068 -1.140396 0.732790 0.140225 0.447638 -0.300207 0.683731 1.305107 -0.254543 -0.764168 -0.910130 -0.115137 0.599016 -0.221136 0.185803 -0.164169 0.252503 0.266733 -0.462284 -0.784074 -0.278293 1.564716 0.298409 -0.040715 -0.231818 -1.695366 -1.180081 -0.543994 -0.992352 0.945524 0.656103 0.774580 1.050712 -0.527359 -0.924795 -0.664498 0.301753 -1.133615 -0.155607 1.208621 -0.536916 -0.541561 -0.708639 -0.692199 1.010675 0.449970 -1.459849 -0.577444 1.090993 0.736170 1.046415 -0.893603 -0.838826 -1.091353 -1.485954 0.735690 -1.337105 0.163299 -0.993690 -0.346670 -0.394804 -0.269896 0.018069 -0.350115 -1.052785 0.078925 -0.054179 0.089335 -1.184984 0.601222 -1.188194 -0.494940 0.285107 1.108264 -0.730086 -0.142099 -0.812729 -0.119301 -0.086066 -1.390560 0.114441 0.699268 0.331129 0.044558 0.593721 0.143837 -2.032076 -0.712676 1.601136 -0.867992 1.300345 -0.869418 -0.400806 -0.833833 -0.803822 -0.977522 -0.182657 -0.328006 -0.790862 -0.243856 -0.132139 0.680105 -0.845089 -1.591178 0.865217 -0.477614 -0.916496 -0.457662 -0.874615 0.093008 -1.459847 -0.174415 -0.603339 -0.899342 -1.766855 0.378189 0.829216 -0.352050 -0.738368 -0.234985 0.227672 0.033258 -0.040866 -0.354544 0.643228 0.056971 -1.085748 0.431833 -0.214974 -0.888970 0.134371 1.024860 0.496303 0.788680 -0.907708 -0.030278 0.901808 -0.405181 -0.093830 0.493759 -0.307850 0.242923 0.010855 0.629267 0.297124 0.782283 -0.128586 0.662886 -0.304470 -0.070713 -0.367860 -0.237940 -0.355662 -1.018032 0.517664 -1.249640 0.337608 0.476555 -1.772337 0.366401 0.159866 0.386786 -0.301655 0.723136 -0.428832 -0.527840 -0.100378 -0.251354 -0.729593 -0.145070 0.089316 0.049480 -0.734847 -0.486519 0.042274 -0.293075 0.429725 -0.504890 0.071848 0.530495 -0.241599 0.432962 -0.608166 -1.324389 1.012171 1.145930 -0.438326 -0.409989 -0.037722 -1.162897 0.391584 -0.853759 0.188189 -0.402611 0.469117 0.459236 0.262606 0.578789 0.037695 0.139978 0.410937 1.413248 0.037847 -0.265542 0.395986 1.152286 -0.291847 0.285738 -1.406926 -0.884878 0.247883 1.087034 0.277790 0.942425 -0.723192 -0.661966 0.327862 -0.037064 0.657196 -0.687922 -0.841417 -0.449857 -0.475873 -0.172681 -0.974939 -0.421336 0.298958 0.589235 0.849994 -0.429038 -0.962805 0.553247 -0.553708 1.045528 0.225277 0.433760 -0.635796 0.253273 0.445246 -0.395951 -0.572543 0.560440 -0.734901 -0.266818 -0.152413 0.200830 -1.351051 -1.523455 1.792502 -1.239867 -0.216845 -0.888174 -0.875459 -0.428093 0.876914 -1.000183 -1.138104 -0.420450 -0.516346 -0.191265 1.204087 -0.369059 -0.524556 0.753534 0.059196 1.238778 0.182618 0.558906 0.269417 0.133730 1.404487 0.194583 0.303193 -0.128805 0.910100 -0.187055 0.488000 0.688441 0.546340 -0.348177 -0.653239 -1.334527 0.801907 0.676567 0.093187 -1.483819 -0.370046 1.073401 -1.122308 0.777719 0.347973 0.677496 0.197087 -0.236071 -0.071673 -0.901841 0.230226 0.100784 0.582711 0.565131 -1.217442 0.622269 -0.786523 1.095089 -0.191861 1.552378 -0.715677 -0.164417 -0.064593 -0.694036 -0.905631 0.183672 0.013247 -0.779979 -1.962507 0.728266 0.646106 -0.235073 0.725377 -1.061749 1.355908 -1.430690 0.256323 -0.086658 0.623606 0.024695 -0.025743 0.606381 0.413663 -1.479421 -0.928931 0.371801 -0.564890 -0.172736 0.256064 0.411688 0.620071 -1.816384 1.568866 0.558701 0.814972 0.312522 -0.460619 -1.784144 -0.672630 -0.318472 -0.597859 -0.539245 1.032001 -1.425445 0.592872 -0.040053 0.945988 -0.207703 1.291401 0.811170 -0.211995 0.321126 -0.280667 0.557219 -0.603572 0.130553 0.363379 -1.911636 -0.140960 -0.024250 -1.035608 0.563488 -0.170384 -1.232881 -2.960544 -1.295353 -1.705213 0.552903 0.635561 -0.157622 0.035941 -0.117382 0.848549 -0.189214 -0.632180 -1.433064 -0.143088 -0.109893 0.607236 1.332671 0.483562 0.649226 -0.303578 -1.889734 -0.449406 0.517495 -0.855861 -0.347152 0.026679 0.415508 -0.719577 1.159387 0.320631 -0.324929 0.232546 0.019007 -0.025747 -0.001279 -1.313303 0.466446 -1.135330 1.565648 -1.001877 -0.042839 -1.512569 1.378626 1.750626 1.288344 0.903387 -0.068654 1.387476 0.691848 0.443788 0.729958 2.087962 0.508066 0.380026 -0.429467 0.925211 1.023353 1.356534 -0.173698 -0.365604 -0.929362 0.575217 0.939907 0.643916 -0.243988 0.233520 -0.363382 0.026648 0.042597 -0.884516 -0.533825 2.188862 -0.152828 1.476890 1.243000 0.536456 -0.413860 0.645581 1.475506 -0.409869 0.040999 -0.531807 -0.467379 -1.541605 0.485261 0.071908 -0.630805 1.487715 -1.624859 0.104132 0.180600 -0.470163 0.200503 -0.164917 -0.097041 -0.524755 0.476295 0.523209 -0.299552 0.244802 -1.107961 1.755257 0.430564 0.803498 0.560031 -0.327402 -0.339697 -1.012934 0.217970 0.430439 -0.526036 -0.102059 1.222802 -0.120143 -0.608819 1.039936 -0.662727 0.146113 0.686685 0.404233 0.177753 0.472169 -0.188145 -0.382779 -1.142101 -0.479599 0.213202 0.321400 -0.151123 0.273612 0.256204 -0.780272 1.365796 0.004096 -0.184226 0.599217 -0.099245 0.331557 -1.152920 0.239953 -0.895805 0.087248 -0.779157 0.228745 -1.463001 0.027251 -0.744563 -0.186390 0.218742 1.398535 -0.908769 -0.068246 -0.760038 1.109152 0.419301 0.301073 -1.468736 -0.513652 0.307661 0.755196 -1.003893 0.164191 -0.272572 0.938291 -0.740236 -0.902774 0.536161 -0.196726 -1.022018 0.477288 0.579435 0.469815 0.207636 -0.308836 -0.986556 -0.068188 0.486577 0.347996 0.617841 0.518259 -0.388711 -0.208878 0.560854 0.189494 1.247574 -0.577795 -0.311635 0.252824 -0.704312 -1.345983 -0.628159 1.135144 0.095212 0.017811 -0.039300 0.291275 0.578898 -0.091957 -0.470865 0.042238 -0.664412 0.903355 0.120059 0.224489 0.492146 -0.122887 -1.111162 0.479270 0.401861 -0.718198 0.122569 -0.202096 -0.031970 1.424700 -1.243590 0.360911 1.486586 1.009188 0.537326 1.285445 -0.241611 -0.424598 0.229980 0.050853 0.221442 -0.603029 -0.064943 -0.067037 0.744057 -0.387321 -0.057677 -0.280217 0.659869 0.551720 -1.061987 0.728494 -0.271476 0.135151 1.073273 -0.770707 -0.749449 1.745448 1.346904 -1.190925 -0.927569 0.994699 -0.430648 0.324750 0.023732 0.044192 0.600983 -0.072837 0.060692 0.058615 0.108684 -0.018796 0.120883 0.074875 0.095192 -0.139967 0.071000 0.040753 -0.055921 0.162097 0.120239 0.106334 0.080821 0.112556 -0.073782 0.028192 0.012962 -0.175557 0.050983 0.120690 0.027185 0.010464 0.061560 0.184353 -0.005548 0.031100 0.162911 -0.107977 -0.014973 0.019576 0.138283 -0.024851 -0.028332 0.015764 0.124684 0.125418 0.126195 0.335316 0.601532 -0.225570 0.204179 0.093359 0.728889 -0.477971 -0.669064 -0.479272 -1.295452 0.402480 -1.077810 0.431067 0.177941 0.448753 0.604028 -0.133843 0.599605 -0.191644 0.529900 0.283645 0.121513 0.997129 0.464684 -0.095282 1.159789 -0.507910 -1.029429 -0.043131 -1.744643 0.370740 -0.042711 -0.230141 -0.014228 0.015970 0.745741 -0.915688 0.149748 -0.122371 -0.289545 -0.612641 -1.188664 -0.086155 -0.937011 -0.681840 -1.403085 -0.682558 0.109934 -0.344020 -1.390074 -0.108572 -0.445911 -0.128816 -0.219641 -0.144243 -2.018532 -0.269484 -1.675890 -0.619268 -0.335678 -0.778807 0.549830 -1.579789 -1.482453 -0.566761 -1.499336 -0.597437 -0.613513 0.672339 -0.359932 -0.576369 -0.436805 -0.440093 -0.234592 -0.524709 -0.159392 -1.773376 -1.054241 -0.239665 -0.505142 -0.281146 -0.691165 -1.620945 1.458724 -0.426447 -1.033255 -0.221071 0.093318 0.707572 1.101801 0.048209 0.628936 0.755606 -0.096178 0.643855 -1.419346 0.398372 -1.091944 0.368711 0.067957 -0.142184 0.012709 -1.902348 -0.132529 -0.009308 -1.242359 0.224290 0.351736 -1.351903 -0.880048 -0.287539 -0.062876 -0.562303 -0.247124 -0.628715 -0.862476 -0.227175 -1.016169 -0.429282 -1.256457 -0.478924 0.579619 -0.770156 -1.164538 -0.305601 1.028226 -0.321597 -0.551880 0.437077 -1.073984 -0.385797 0.191459 -0.141433 -0.751005 -0.445836 -2.958195 -0.645799 0.514957 -0.476586 -0.113917 0.659961 0.578396 1.410421 -0.111671 0.490199 0.713413 0.256344 0.129825 -1.149935 -0.910218 0.332985 0.984264 0.712359 -0.355340 0.507884 -0.298170 0.632706 0.268523 -0.509234 1.024943 0.040324 -1.693170 -1.397058 -0.513553 0.010533 0.402597 -0.537415 0.954632 -0.375248 1.453483 0.249793 -0.389785 0.241287 0.382272 0.149728 -1.001194 -0.110176 -0.637974 -0.237784 -0.511570 0.024910 -0.391134 -1.469355 0.356511 0.393946 0.138808 -2.025776 0.033230 -2.118185 -0.102492 -0.784458 0.117683 -0.396047 -0.288976 -0.076752 -3.078173 -0.485129 -0.646660 -0.089815 -1.156899 0.816930 0.576450 -2.154649 -0.000439 0.720086 0.148772 -0.900437 0.367440 1.172485 0.630045 0.459988 0.336906 0.497128 -0.285578 0.120668 -1.069310 0.323860 0.384837 1.365999 -0.828291 0.818706 -0.411680 1.873867 0.920989 0.557397 0.852106 0.965912 -0.086632 0.116478 0.170359 0.553365 0.183818 0.525938 -0.466091 0.331345 0.697790 0.514826 0.534522 -0.660238 0.209311 1.064359 0.072638 1.374324 -0.211964 1.213568 -1.374644 0.223171 -1.079568 0.334955 -0.790625 0.120328 0.242672 0.170142 -0.318453 0.165298 1.344340 -0.099946 0.789424 -0.282366 0.218079 -0.271045 0.021515 0.581843 -1.004636 -0.186954 0.966714 0.606617 -0.890828 0.543853 1.516569 -0.198057 0.198798 -0.288330 0.232008 -0.549102 0.043688 0.427215 0.885502 0.603222 -0.129294 -0.010731 -0.978138 -0.029380 -0.727891 -0.223174 -0.922895 -0.730133 -0.925307 -0.284445 -0.178615 0.183389 -0.556959 -0.706681 -0.859137 -0.559121 -2.525675 0.402314 -0.664169 -0.940437 -1.887403 -0.053108 0.626465 0.125612 -0.520365 -0.373370 -0.343090 -0.017428 -1.268428 -1.074986 -1.146041 0.184440 -0.237799 -0.164885 -0.373510 -0.793914 0.209640 -0.364232 0.159521 -1.090024 -0.219836 -0.724163 0.819783 -1.467972 -0.352026 -0.880384 0.513375 0.811434 -0.146796 -0.641715 1.186344 -0.214232 0.054523 0.367458 0.347453 0.187608 0.666041 -0.006595 0.784836 -0.175394 -0.091017 0.459537 0.707003 -1.761030 0.482231 0.712724 0.471361 0.505575 1.510548 0.138601 0.470069 -0.062319 0.837886 -0.531778 0.791758 -0.471700 1.348321 0.218542 1.170951 0.210533 -0.011134 0.259873 0.167212 0.649337 -0.116258 -0.171432 -0.135284 -0.368086 -1.176913 -0.045322 -0.661707 0.430518 -0.050708 0.239685 0.124344 0.122388 1.320062 -0.810384 -1.116891 0.456653 -0.534032 -0.833313 -0.562079 -1.852819 -0.746339 -0.725202 -0.665726 0.060520 0.038749 0.878349 0.193264 -0.409171 0.098168 -0.389102 0.260041 -0.333052 -0.523451 0.902035 -0.663793 -0.252112 -1.035325 -0.044040 0.499162 0.009303 -1.197394 0.232247 -0.230465 -0.969829 1.484420 0.965720 -0.303537 -0.180945 -0.650367 -0.186681 0.625293 -0.448082 0.688263 0.351830 0.395141 0.555006 0.235210 -0.061977 0.075160 0.683285 -0.698517 -0.159323 -0.699508 0.021966 -1.452542 -1.073683 -1.298144 -0.010924 -0.475221 -0.100161 -0.593752 -0.133306 -0.210757 0.125063 -0.144655 -0.500080 -0.369018 -0.167009 0.055379 1.371307 0.250714 -0.134893 -0.245136 -0.052974 -0.523988 0.319243 1.681083 -0.159036 0.732762 0.204143 0.017971 0.184600 0.088088 0.710334 -0.193336 0.240597 1.379988 -0.240644 -2.602532 0.618878 -0.431750 -0.260048 0.264059 0.415126 -1.419698 -0.836302 0.231395 -0.112299 0.008424 -0.479373 -0.174378 0.011796 0.258354 -0.444454 -1.066492 -0.816873 1.016739 -0.432135 -0.490573 0.693642 -0.152540 -0.486155 -0.320058 -0.028756 1.523379 0.446392 1.155704 -0.117198 0.256957 0.331279 0.334904 0.325347 -0.563218 0.160226 -0.099151 0.036747 1.346129 0.438497 0.555950 -1.135590 -0.011437 0.301820 0.829891 -0.562290 0.759157 0.183811 -0.330752 0.274553 -0.513407 0.353503 0.038237 0.433331 0.486791 -1.307820 0.622063 0.065899 0.984291 -0.596167 -0.804370 0.096441 0.520785 -0.348537 0.257820 0.473048 -1.043979 0.148802 -1.465878 -0.538591 0.086037 -0.098937 -0.481139 -0.191744 1.445445 -0.578217 0.153521 0.510446 -0.616701 0.491417 0.598950 -2.170119 -0.514011 0.351774 0.371177 0.159017 0.905271 0.608754 -1.165728 0.493180 -0.794797 -0.922807 -0.338099 -0.621745 -1.076836 -2.108442 0.240924 -0.074157 -0.795068 -0.868897 -0.569874 -1.664389 0.090909 -0.486383 -1.489409 -0.226158 -0.381158 0.543482 -0.602044 -0.757297 0.330218 -0.807844 -0.363147 -0.516317 -0.856874 -0.940312 -0.389701 0.436512 -0.561748 -0.128822 -0.279063 1.203881 -0.455886 -0.210986 0.212926 -0.102850 0.970674 -0.792366 0.019332 0.008640 0.854055 -0.316376 -0.337335 -0.243672 2.085563 -1.353020 1.274765 -0.487361 0.644640 0.171161 0.756038 1.605583 -0.356068 0.073487 0.761191 1.224588 0.685675 -0.555993 1.779717 0.179754 0.105064 0.980400 0.014793 0.715240 -1.172376 -0.010911 -0.442883 -0.073870 0.468739 -0.056704 -0.029254 1.061870 0.287209 0.202501 -2.144130 -1.237804 -1.013260 -0.006172 0.177968 -0.220971 -1.178218 -0.249271 0.438241 -0.050779 -0.354250 -1.373004 -0.595945 0.412193 0.094620 -0.289717 1.258088 -0.013717 -0.612882 -0.095521 -0.062333 -0.157210 -0.937163 -0.938857 -1.498729 -0.270980 -0.810374 -0.176676 -0.212173 0.078263 1.055143 1.635368 -0.249392 -0.596411 -0.569345 -0.447577 -1.108130 1.441684 -0.001662 -0.047341 -0.756200 0.839653 -0.075279 -0.874130 1.182704 -0.647211 0.489653 -0.074390 -0.026785 0.042310 -0.691788 0.336690 -1.740703 -0.096706 0.348844 0.028835 0.005257 0.788684 0.694180 -0.044103 -0.495782 -0.344570 1.263979 1.003012 -0.536148 0.058777 -0.753064 -0.436560 0.853095 -0.404627 0.951352 -0.654987 -0.042920 -0.573996 -0.005794 -0.294998 0.341944 -0.876802 -0.078580 0.121702 -0.824640 -0.164325 1.168448 -0.056627 1.066906 -0.560863 0.102341 0.528723 0.302617 0.497151 -0.560539 1.049471 0.059321 -0.327992 0.399683 0.092751 0.302310 0.542659 -0.241699 -0.244262 -0.275797 0.012791 -1.000374 -2.128815 1.618511 0.265004 0.855325 0.916633 0.057237 0.797609 0.452216 0.304083 -0.106596 -0.835820 -0.159735 0.280017 0.711749 -0.361980 0.065419 0.263148 -0.206992 -0.346710 1.721789 0.390920 0.615983 0.134180 0.249318 -0.062698 0.147294 0.132655 0.408149 -0.470042 -0.421894 -0.104105 -1.146761 0.240562 -1.017907 1.674010 0.817621 -0.028244 1.564309 0.774585 -0.287404 0.000643 1.103511 0.292132 0.174455 0.497108 -0.018925 0.434724 -0.278449 1.137208 -0.076189 -0.231105 1.214757 0.423931 -0.570253 0.590743 -0.601581 0.100188 0.348897 0.315871 -1.246258 -0.618512 -1.037382 0.428083 -0.745630 -0.253298 -0.498156 -0.678569 0.335508 0.015687 0.050206 -0.268942 -0.675995 -0.030124 -0.086019 0.163284 -0.134258 -0.127116 -0.092975 -0.014739 0.261584 -0.013386 0.136983 0.261155 0.124537 -0.051123 0.255448 -0.108616 -0.007830 -0.053529 -0.025291 0.060630 0.136699 0.250134 -0.000081 1.606011 0.000437 2.773097 -0.000223 -0.003356 1.440230 -0.180235 0.019775 2.314371 0.005780 0.386832 -0.000777 0.002663 1.871331 0.039086 1.682065 -0.001213 1.240035 -0.183312 -0.008238 -0.006610 0.232720 0.184033 0.008311 0.004482 0.413867 -0.269605 -0.007476 1.163949 0.000702 -0.161450 1.770181 -0.997336 0.030783 -0.095293 -2.614638 0.654681 0.001675 0.101722 0.362481";
    
    // NeuralNetwork nn({INPUT, 30, 40, 20, OUTPUT}, {TANH, RELU, RELU, TANH}, 0.001, 0.50);
    NeuralNetwork nn(ss);
    cin >> checkpoints; cin.ignore();
    cerr << checkpoints << '\n';

    for(int i = 0; i < checkpoints; i++) {
        cin >> cp[i].x >> cp[i].y; cin.ignore();
        cerr << cp[i].x << ' ' << cp[i].y << '\n';
    }

    static float input[INPUT];
    static float output[OUTPUT];

    while(true) {
        int cp_id;
        int x;
        int y;
        int vx;
        int vy;
        int ang; 

        cin >> cp_id >> x >> y >> vx >> vy >> ang; cin.ignore();
        cerr << cp_id << ' ' << x << ' ' << y << ' ' << vx << ' ' << vy << ' ' << ang << '\n';

        vec2 pod(x, y);

        vec2 v(x, y);

        double angle_to_cp1 = get_angle(pod, cp[cp_id]);

        int D = dist(pod, cp[cp_id]);
        double angle = ang * pi / 180 + angle_to_cp1;
        double angleV = atan2(vy, vx) + angle_to_cp1;
        double normV = v.len();
        double angleNextDir = get_angle(pod - cp[cp_id], cp[(cp_id + 1) % checkpoints] - cp[cp_id]);
        double distanceCp1Cp2 = dist(cp[cp_id], cp[(cp_id + 1) % checkpoints]);

        cerr << "D: " << D << '\n';
        cerr << "angle: " << angle << '\n';
        cerr << "angleV: " << angleV << '\n';
        cerr << "normV: " << normV << '\n';
        cerr << "angleNextDir: " << angleNextDir << '\n';
        cerr << "distanceCp1Cp2: " << distanceCp1Cp2 << '\n';

        double angleScalD = dot(vec2(angle), vec2(1.0,0.0));
        double angleCrossD = cross(vec2(angle), vec2(1.0,0.0));
        int vScalD = (int)(normV*dot(vec2(angleV), vec2(1.0,0.0)));
        int vCrossD = (int)(normV*cross(vec2(angleV), vec2(1.0,0.0)));
        double nextDirScalD = dot(vec2(angleNextDir), vec2(1.0, 0.0));
        double nextDirCrossD = cross(vec2(angleNextDir), vec2(1.0, 0.0));

        input[0] = (D - 600) / 20000.0;
        input[1] = angleScalD;
        input[2] = angleCrossD;
        input[3] = vScalD / 1000.0;
        input[4] = vCrossD / 1000.0;
        input[5] = nextDirScalD;
        input[6] = nextDirCrossD;
        input[7] = (distanceCp1Cp2 - 1200.0) / 10000.0;

        nn.forward(input, output);

        int res_ang = int(output[0] * 36 - 18 + 0.5);
        int res_thr = int(output[1] * 200 + 0.5);
        
        if(res_ang < -18)   res_ang = -18;
        if(res_ang > +18)   res_ang = +18;

        cout << "EXPERT " << res_ang << ' ' << res_thr << endl;
    }
}

/*
// Section 1 : distribution of random gamestates used for this database
int D = 602+fastRandInt(0, 10000)*sqrt(fastRandDouble()); // Distance between pod and CP1 (his next CP)
double angle = fastRandDouble(-PI, PI)*sqrt(fastRandDouble());	// Assuming angle 0 is "straight to CP1", this is where the pod looks
double angleV = fastRandDouble(-PI, PI)*sqrt(fastRandDouble());// Assuming angle 0 is "straight to CP1", this is angle of the normalized pod     velocity
double normV = fastRandDouble()*fastRandDouble()*1000.0; // Current speed of the pod
double angleNextDir = fastRandDouble(-PI, PI);// Assuming angle 0 is "straight to CP1", this is the angle between (pod->CP1)     and (CP1->CP2)
double distanceCP1CP2 = fastRandInt(1300, 10000)*sqrt(fastRandDouble());// Distance between CP1 and CP2

// Section 2 : features calculated from this distribution
double angleScalD = dot(vec2ByAngle(angle), vec2(1.0,0.0));
double angleCrossD = cross(vec2ByAngle(angle), vec2(1.0,0.0));
int vScalD = (int)(normV*dot(vec2ByAngle(angleV), vec2(1.0,0.0)));
int vCrossD = (int)(normV*cross(vec2ByAngle(angleV), vec2(1.0,0.0)));
double nextDirScalD = dot(vec2ByAngle(angleNextDir), vec2(1.0, 0.0));
double nextDirCrossD = cross(vec2ByAngle(angleNextDir), vec2(1.0, 0.0));

// Section 3 : data from section 1 and 2 is normalized this way before being provided to the neural network as input (saved in the database on one line with space separators)
(D-600)/20000.0 
angleScalD
angleCrossD
vScalD/1000.0
vCrossD/1000.0
nextDirScalD
nextDirCrossD
(distanceCP1CP2 - 1200.0) / 10000.0

// Section 4 : same as section 3, but for the output (saved in the database on one line with space separators)
(gameAction.deltaAngle*180/PI+18.0)/36.0
gameAction.thrust/200.0

// Miscellaneous : 
dot(const vec2 &a, const vec2 &b) {
  return a.x*b.x+a.y*b.y;
}
cross(const vec2 &vec, const vec2 &axe) {
  return vec.y*axe.x-vec.x*axe.y;
}
*/

/*
inline double dot(const vec2 &a, const vec2 &b) {
  return a.x*b.x+a.y*b.y;
}
inline double cross(const vec2 &vec, const vec2 &axe) {
	//projeté de vec sur la direction orthogonale à axe, à +90°
  return vec.y*axe.x-vec.x*axe.y;
}

int D = 602+fastRandInt(0, 10000)*sqrt(fastRandDouble());
double angle = fastRandDouble(-PI, PI)*sqrt(fastRandDouble());
double angleV = fastRandDouble(-PI, PI)*sqrt(fastRandDouble());
double normV = fastRandDouble()*fastRandDouble()*1000.0;
double angleNextDir = fastRandDouble(-PI, PI);
double distanceCP1CP2 = fastRandInt(1300, 10000)*sqrt(fastRandDouble());

//Données pour le réseau de neurones - Dane dla sieci neuronowej
//int D;
double angleScalD = dot(vec2ByAngle(angle), vec2(1.0,0.0));
double angleCrossD = cross(vec2ByAngle(angle), vec2(1.0,0.0));
int vScalD = (int)(normV*dot(vec2ByAngle(angleV), vec2(1.0,0.0)));
int vCrossD = (int)(normV*cross(vec2ByAngle(angleV), vec2(1.0,0.0)));
double nextDirScalD = dot(vec2ByAngle(angleNextDir), vec2(1.0, 0.0));
double nextDirCrossD = cross(vec2ByAngle(angleNextDir), vec2(1.0, 0.0));


myWriteFile << (D-600)/20000.0<< "  "<<angleScalD<< "  "<<angleCrossD<< "  "<<vScalD/1000.0<< "  "<<vCrossD/1000.0<< "   "<<nextDirScalD<< "  "<<nextDirCrossD << "  " << (distanceCP1CP2 - 1200.0) / 10000.0 << std::endl;
myWriteFile << "  "<< (gameAction.deltaAngle*180/PI+18.0)/36.0 << "  " << gameAction.thrust/200.0<<std::endl;






Below : all the 99999 datapoints with 8 inputs and 2 outputs, already normalized.
Above, : the sampling procedure, and the normalization procedure to obtain the data below.
The data was obtained by running my "main" bot (using a genetic algorithm) to find an optimal trajectory towards the next checkpoint.



99999 8 2

*/

/*
    
    // ifstream data_file;
    // data_file.open("data/CSB_Data_to_train_NN.data");

    // for(int i = 0; i < DATA; i++) {
    //     for(int j = 0; j < INPUT; j++) {
    //         data_file >> Data[i].first[j];
    //     }

    //     for(int j = 0; j < OUTPUT; j++) {
    //         data_file >> Data[i].second[j];
    //     }
    // }

    // data_file.close();

    // float best_score = 1782.624268;
    // for(int epoch = 0; epoch < 10000; epoch++) {
    //     std::random_shuffle(Data.begin(), Data.end());
    //     for(int sample = 0; sample < DATA; sample++) {

    //         static float output[OUTPUT];
    //         nn.forward(&Data[sample].first[0], output);
    //         nn.backprop(&Data[sample].second[0]);
    //     }

    //     float error = 0;
    //     for(int i = 0; i < DATA; i++) {

    //         static float output[OUTPUT];

    //         nn.forward(&Data[i].first[0], output);

    //         for(int j = 0; j < OUTPUT; j++) {
    //             float delta = output[j] - Data[i].second[j];
    //             error += delta * delta;
    //         }
    //     }

    //     cerr << "sum error: " << error << '\n';
    //     cerr << "avg error: " << error / DATA << "\n";
    //     cerr << "\n";

    //     if(error < best_score) {
    //         best_score = error;
    //         string file_name = "search_race_nn/search_race(" + to_string(error) + ").nn";
    //         nn.save(&file_name[0]);
    //     }
    // }

    // nn.save("search_race_nn/search_race.nn");

*/
// *** End of: /home/olaf/code/NeuralNet/search_race.cpp *** 
